{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8174ebba-d58d-4585-8744-5b7a243075fa",
   "metadata": {},
   "source": [
    "* Issue [37](https://github.com/salgo60/SCB-Wikidata/issues/37)\n",
    "* denna Notebook [SCB_37_myndighetsregister.ipynb](https://github.com/salgo60/SCB-Wikidata/blob/main/notebook/SCB_37_myndighetsregister.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bbc668e-ee9d-4322-9fe0-f21724aa1113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2025-11-20 13:03:52\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "timestamp = now.timestamp()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Start:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0252b730-458e-4480-85e7-5d2dd5e4558b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Läste in 250 rader från SCB-filen\n",
      "Kolumner: ['Organisationsnr', 'Namn', 'PostAdress', 'PostNr', 'PostOrt', 'BesöksAdress', 'BesöksPostNr', 'BesöksPostOrt', 'Tfn', 'Fax', 'Epost', 'Webbadress', 'SFS']\n",
      "Unika orgnummer: 250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hämtar från Wikidata: 100%|███████████████████████| 5/5 [00:02<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matchade 248 organisationsnummer i Wikidata\n",
      "Poster i Wikidata men saknas i SCB: 2953\n",
      "Filer skapade:\n",
      "myndighetsreg/MR_enriched_251120_133015.csv\n",
      "myndighetsreg/MR_no_match_251120_133015.csv\n",
      "myndighetsreg/MR_discrepancies_251120_133015.csv\n",
      "myndighetsreg/Wikidata_not_in_SCB_251120_133015.csv\n",
      "Totalt poster: 250\n",
      "Hittade i Wikidata: 0\n",
      "Namn-avvikelser: 250\n",
      "Webb-avvikelser: 246\n",
      "Ej matchade i Wikidata: 250\n",
      "Filer sparade: myndighetsregistret_enriched.csv, myndighetsregistret_no_wikidata_match.csv, myndighetsregistret_discrepancies.csv\n",
      "Klar\n"
     ]
    }
   ],
   "source": [
    "# Notebook: Myndighetsregistret -> Wikidata matching\n",
    "# File source (local): /mnt/data/Statliga förvaltningsmyndigheter.txt\n",
    "\n",
    "# Syfte:\n",
    "# 1. Läs in SCB:s nedladdade fil (Statliga förvaltningsmyndigheter)\n",
    "# 2. Normalisera organisationsnummer\n",
    "# 3. Fråga Wikidata (P6460 Swedish Organization Number) för matchande poster\n",
    "# 4. Jämför värden (namn, webb, epost etc.) och markera avvikelser\n",
    "# 5. Skriv ut rapporter (CSV/JSON)\n",
    "\n",
    "# Instruktion: kör cellerna i en Jupyter Notebook-miljö (eller kör som script). Alla paths är relativa till körmiljön.\n",
    "\n",
    "# --- Installera beroenden (avkommentera vid behov) ---\n",
    "# !pip install pandas requests tqdm SPARQLWrapper\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Organisationsnumret är det primära ID:t i myndighetsregistret och motsvarar direkt Wikidata-property wdt:P6460\n",
    "\n",
    "# --- 1. Läs in SCB-filen ---\n",
    "SCB_FILE = 'myndighetsreg/Statliga förvaltningsmyndigheter.txt'  # <- lokal fil som du laddat upp\n",
    "\n",
    "# Filen ser ut att vara tab-separerad med en header-rad\n",
    "df = pd.read_csv(SCB_FILE, sep='\\t', dtype=str, encoding='utf-8')\n",
    "print(f\"Läste in {len(df)} rader från SCB-filen\")\n",
    "\n",
    "# Rensa: standardisera kolumnnamn (trimma och gör enklare namn)\n",
    "orig_cols = df.columns.tolist()\n",
    "colmap = {c: c.strip() for c in orig_cols}\n",
    "df.rename(columns=colmap, inplace=True)\n",
    "\n",
    "# Visa kolumner\n",
    "print('Kolumner:', list(df.columns))\n",
    "\n",
    "# --- 2. Normalisera organisationsnummer ---\n",
    "# Normalisera organisationsnummer till formatet YYYYMM-NNNN (krävs för Wikidata P6460)\n",
    "\n",
    "def normalize_orgnr(v):\n",
    "    if pd.isna(v): return None\n",
    "    s = str(v).strip().replace(' ', '')\n",
    "    digits = re.sub(r\"[^0-9]\", \"\", s)\n",
    "    if len(digits) == 10:\n",
    "        return digits[:6] + '-' + digits[6:]\n",
    "    return s\n",
    "\n",
    "df['orgnr_norm'] = df['Organisationsnr'].apply(normalize_orgnr)\n",
    "\n",
    "# --- 3. Hämta matchningar från Wikidata --- Hämta matchningar från Wikidata ---\n",
    "# Wikidata property för Swedish Organization Number är P6460\n",
    "SPARQL_ENDPOINT = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "sparql = SPARQLWrapper(SPARQL_ENDPOINT)\n",
    "sparql.setReturnFormat(JSON)\n",
    "\n",
    "# Funktion som frågar Wikidata med en chunk av organisationsnummer (plain, utan bindestreck) \n",
    "def query_wikidata_for_orgnrs(orgnr_list):\n",
    "    if not orgnr_list:\n",
    "        return {}\n",
    "\n",
    "    vals = \" \".join(f'\"{o}\"' for o in orgnr_list)\n",
    "\n",
    "    q = \"\"\"\n",
    "    SELECT ?item ?itemLabel ?orgnr ?site ?website WHERE {\n",
    "      VALUES ?orgnr { %s }\n",
    "      ?item wdt:P6460 ?orgnr .\n",
    "      OPTIONAL { ?item wdt:P856 ?website . }\n",
    "      OPTIONAL { ?item wdt:P973 ?site . }\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"sv,en\". }\n",
    "    }\n",
    "    \"\"\" % vals\n",
    "\n",
    "    sparql.setQuery(q)\n",
    "    ...\n",
    "\n",
    "    # följsamhet: sätt User-Agent\n",
    "    sparql.addCustomHttpHeader('User-Agent', 'SCB-Wikidata-Matcher/1.0 (your@email.example)')\n",
    "    res = sparql.query().convert()\n",
    "    out = {}\n",
    "    for b in res['results']['bindings']:\n",
    "        org = b['orgnr']['value']\n",
    "        item = b['item']['value']\n",
    "        label = b.get('itemLabel', {}).get('value')\n",
    "        website = b.get('website', {}).get('value')\n",
    "        site = b.get('site', {}).get('value')\n",
    "        out[re.sub(r'[^0-9]', '', org)] = {\n",
    "            'wikidata_item': item,\n",
    "            'label': label,\n",
    "            'website': website,\n",
    "            'site': site\n",
    "        }\n",
    "    return out\n",
    "\n",
    "# Kör i batchar (Wikidata SPARQL kan hantera ett rimligt antal VALUES — kör t.ex. 50 åt gången)\n",
    "#orgnrs = df['orgnr_norm'].dropna().unique().tolist().dropna().unique().tolist()\n",
    "\n",
    "orgnrs = (\n",
    "    df['orgnr_norm']\n",
    "    .dropna()\n",
    "    .unique()\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "print(f\"Unika orgnummer: {len(orgnrs)}\")\n",
    "\n",
    "batch_size = 50\n",
    "wikidata_map = {}\n",
    "for i in tqdm(range(0, len(orgnrs), batch_size), desc='Hämtar från Wikidata'):\n",
    "    chunk = orgnrs[i:i+batch_size]\n",
    "    try:\n",
    "        res = query_wikidata_for_orgnrs(chunk)\n",
    "        wikidata_map.update(res)\n",
    "    except Exception as e:\n",
    "        print('SPARQL-fel vid chunk', i, e)\n",
    "        time.sleep(5)\n",
    "\n",
    "print(f\"Matchade {len(wikidata_map)} organisationsnummer i Wikidata\")\n",
    "\n",
    "# --- 4. Slå ihop data och hitta avvikelser ---\n",
    "# Lägg in matchningskolumner i df\n",
    "\n",
    "def lookup_wikidata(plain):\n",
    "    if pd.isna(plain):\n",
    "        return None\n",
    "    return wikidata_map.get(plain, None)\n",
    "\n",
    "df['wikidata'] = df['orgnr_norm'].apply(lambda x: lookup_wikidata(x))\n",
    "\n",
    "# Extrahera kolumner från wikidata-dict\n",
    "\n",
    "df['wd_item'] = df['wikidata'].apply(lambda x: x['wikidata_item'] if isinstance(x, dict) else None)\n",
    "df['wd_label'] = df['wikidata'].apply(lambda x: x.get('label') if isinstance(x, dict) else None)\n",
    "df['wd_website'] = df['wikidata'].apply(lambda x: x.get('website') if isinstance(x, dict) else None)\n",
    "\n",
    "def compare_text(a, b):\n",
    "    if pd.isna(a) and pd.isna(b):\n",
    "        return False\n",
    "    if pd.isna(a) != pd.isna(b):\n",
    "        return True\n",
    "    a_s = str(a).strip().lower()\n",
    "    b_s = str(b).strip().lower()\n",
    "    return a_s != b_s\n",
    "\n",
    "# Avvikelse: namn skiljer sig\n",
    "\n",
    "df['avvik_namn'] = df.apply(lambda r: compare_text(r['Namn'], r['wd_label']), axis=1)\n",
    "# Avvikelse: webb skiljer sig (kontrollera delvis match eller olika domän)\n",
    "\n",
    "def website_normalize(u):\n",
    "    if pd.isna(u):\n",
    "        return None\n",
    "    u = str(u).strip()\n",
    "    u = re.sub(r'^https?://', '', u)\n",
    "    u = u.rstrip('/')\n",
    "    return u.lower()\n",
    "\n",
    "df['post_webb'] = df['Webbadress'].apply(lambda x: website_normalize(x) if 'Webbadress' in df.columns else None)\n",
    "df['wd_webb_norm'] = df['wd_website'].apply(website_normalize)\n",
    "df['avvik_webb'] = df.apply(lambda r: compare_text(r['post_webb'], r['wd_webb_norm']), axis=1)\n",
    "\n",
    "# Markera poster utan match i Wikidata\n",
    "ndf = df.copy()\n",
    "ndf['matched_in_wikidata'] = ndf['wd_item'].notna()\n",
    "\n",
    "# --- 5. Dubbelriktad kontroll (SCB → WD och WD → SCB) ---\n",
    "# 5a. SCB → Wikidata: frågor vi redan gjort\n",
    "# 5b. Wikidata → SCB: hämta alla Wikidata‑poster som har P6460 och jämföra mot SCB\n",
    "\n",
    "# Hämta ALLa Wikidata‑poster med P6460 (kan vara många, hämta i batch om nödvändigt)\n",
    "\n",
    "sparql_all = SPARQLWrapper(SPARQL_ENDPOINT)\n",
    "sparql_all.setReturnFormat(JSON)\n",
    "q_all = \"\"\"\n",
    "SELECT ?item ?itemLabel ?orgnr WHERE {\n",
    "  ?item wdt:P6460 ?orgnr .\n",
    "  SERVICE wikibase:label { bd:serviceParam wikibase:language 'sv,en'. }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "sparql_all.setQuery(q_all)\n",
    "sparql_all.addCustomHttpHeader('User-Agent', 'SCB-Wikidata-Matcher/1.0')\n",
    "res_all = sparql_all.query().convert()\n",
    "\n",
    "wikidata_full = {}\n",
    "for b in res_all['results']['bindings']:\n",
    "    org = b['orgnr']['value']\n",
    "    item = b['item']['value']\n",
    "    label = b.get('itemLabel', {}).get('value')\n",
    "    wikidata_full[re.sub(r'[^0-9]', '', org)] = {\n",
    "        'item': item,\n",
    "        'label': label\n",
    "    }\n",
    "\n",
    "# Lista: de som finns i Wikidata men inte i SCB\n",
    "set_scb = set(df['orgnr_norm'].dropna())\n",
    "set_wd = set(wikidata_full.keys())\n",
    "wd_not_in_scb = sorted(list(set_wd - set_scb))\n",
    "\n",
    "print(f\"Poster i Wikidata men saknas i SCB: {len(wd_not_in_scb)}\")\n",
    "\n",
    "wd_missing_df = pd.DataFrame([\n",
    "    {\n",
    "        'orgnr': o,\n",
    "        'wikidata_item': wikidata_full[o]['item'],\n",
    "        'wikidata_label': wikidata_full[o]['label']\n",
    "    } for o in wd_not_in_scb\n",
    "])\n",
    "\n",
    "# --- 6. Rapporter (med tidsstämplade filnamn) ---\n",
    "from datetime import datetime\n",
    "TS = datetime.now().strftime('%y%m%d_%H%M%S')\n",
    "\n",
    "out_all = f'myndighetsreg/MR_enriched_{TS}.csv'\n",
    "out_nomatch = f'myndighetsreg/MR_no_match_{TS}.csv'\n",
    "out_discrep = f'myndighetsreg/MR_discrepancies_{TS}.csv'\n",
    "out_wd_missing = f'myndighetsreg/Wikidata_not_in_SCB_{TS}.csv'\n",
    "\n",
    "ndf.to_csv(out_all, index=False)\n",
    "ndf[~ndf['matched_in_wikidata']].to_csv(out_nomatch, index=False)\n",
    "ndf[ndf['avvik_namn'] | ndf['avvik_webb']].to_csv(out_discrep, index=False)\n",
    "wd_missing_df.to_csv(out_wd_missing, index=False)\n",
    "\n",
    "print('Filer skapade:')\n",
    "print(out_all)\n",
    "print(out_nomatch)\n",
    "print(out_discrep)\n",
    "print(out_wd_missing)\n",
    "\n",
    "# Slut\n",
    "# Skriv ut sammanfattning\n",
    "print('Totalt poster:', len(ndf))\n",
    "print('Hittade i Wikidata:', ndf['matched_in_wikidata'].sum())\n",
    "print('Namn-avvikelser:', ndf['avvik_namn'].sum())\n",
    "print('Webb-avvikelser:', ndf['avvik_webb'].sum())\n",
    "print('Ej matchade i Wikidata:', (~ndf['matched_in_wikidata']).sum())\n",
    "\n",
    "# Spara rapporter\n",
    "ndf.to_csv('myndighetsregistret_enriched.csv', index=False)\n",
    "ndf[~ndf['matched_in_wikidata']].to_csv('myndighetsregistret_no_wikidata_match.csv', index=False)\n",
    "ndf[ndf['avvik_namn'] | ndf['avvik_webb']].to_csv('myndighetsregistret_discrepancies.csv', index=False)\n",
    "\n",
    "print('Filer sparade: myndighetsregistret_enriched.csv, myndighetsregistret_no_wikidata_match.csv, myndighetsregistret_discrepancies.csv')\n",
    "\n",
    "# --- Extra: Förslag på nästa steg ---\n",
    "# - Kör en fuzzy-matchning (fuzzywuzzy / rapidfuzz) mot Wikidata-labels för ej matchade organisationer\n",
    "# - Hämta fler fält från Wikidata (ex. official website, country, inception, dissolved) för att bättre kontrollera\n",
    "# - Spara Wikidata-item till en separat tabell och logga skillnader per fält med referenser\n",
    "\n",
    "# Slut\n",
    "print('Klar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9617c400-1e48-4e38-84b1-1db22c0e364a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2025-11-20 13:04:36\n",
      "Total time elapsed: 00 minutes 44.64 seconds\n"
     ]
    }
   ],
   "source": [
    " # End timer and calculate duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time# Bygg audit-lager för den här etappen\n",
    "\n",
    "# Print current date and total time\n",
    "print(\"Date:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "print(\"Total time elapsed: {:02.0f} minutes {:05.2f} seconds\".format(minutes, seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf0c24-b83b-4bca-9f85-6f81fc25ca52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
