{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74da516d-bdd3-489b-a98d-e6fa71145599",
   "metadata": {},
   "source": [
    "* [#4](https://github.com/salgo60/SCB-Wikidata/issues/4)\n",
    "* denna [SCB4_linkroot.ipynb](https://github.com/salgo60/SCB-Wikidata/blob/main/notebook/SCB4_linkroot.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d311f58-d8a4-42f9-aa33-819e7988c23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2025-11-17 01:39:41\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "timestamp = now.timestamp()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Start:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ce80ab-900c-4569-9baf-9f593c028ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(level, msg):\n",
    "    ts = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{ts}] [{level}] {msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53643e95-b697-4cf4-a62c-262f756ea52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_language_codes(file_path): \n",
    "    print(f\"[DEBUG] Reading language codes from: {file_path}\") \n",
    "    df = pd.read_csv(file_path) \n",
    "    lang_codes = df['Language Code'].dropna().unique().tolist() \n",
    "    print(f\"[DEBUG] Found {len(lang_codes)} language codes.\") \n",
    "    return lang_codes \n",
    "    \n",
    "def read_domains(file_path): \n",
    "    print(f\"[DEBUG] Reading domains from: {file_path}\") \n",
    "    df = pd.read_csv(file_path, header=None, names=['domain']) \n",
    "    domains_list = df['domain'].dropna().unique().tolist() \n",
    "    print(f\"[DEBUG] Found {len(domains_list)} domains.\") \n",
    "    return domains_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd808572-a6a2-4b6c-ac33-4463461076f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alla rader i checkpoint: 110927\n",
      "Totalt unika URL: 23871\n",
      "\n",
      "Språk-kolumn hittad: 'lang'\n",
      "\n",
      "--- Statistik per språk (alla rader) ---\n",
      "sv    71851\n",
      "fi    9316\n",
      "no    6638\n",
      "en    6408\n",
      "pl    2668\n",
      "de    2224\n",
      "it    2122\n",
      "ro    1905\n",
      "da    1818\n",
      "es    1453\n",
      "hu    1193\n",
      "pt    513\n",
      "ru    500\n",
      "tr    395\n",
      "nn    362\n",
      "nl    352\n",
      "is    315\n",
      "vi    226\n",
      "hr    174\n",
      "cs    140\n",
      "ca    96\n",
      "se    79\n",
      "sl    56\n",
      "et    41\n",
      "lv    28\n",
      "eo    19\n",
      "sk    18\n",
      "so    6\n",
      "rm    4\n",
      "cy    3\n",
      "ga    2\n",
      "ceb   2\n",
      "\n",
      "--- Statistik per språk (unika URL:er) ---\n",
      "sv    20931\n",
      "da    1304\n",
      "en    815\n",
      "fi    341\n",
      "de    103\n",
      "es    75\n",
      "ru    56\n",
      "pt    44\n",
      "no    36\n",
      "pl    25\n",
      "it    21\n",
      "nl    17\n",
      "hu    16\n",
      "cs    13\n",
      "ro    12\n",
      "ca    11\n",
      "tr    10\n",
      "hr    7\n",
      "nn    6\n",
      "et    6\n",
      "is    6\n",
      "vi    5\n",
      "lv    2\n",
      "eo    2\n",
      "rm    2\n",
      "sk    2\n",
      "ga    1\n",
      "sl    1\n",
      "so    1\n",
      "\n",
      "Antal språk: 32\n",
      "\n",
      "--- Statistik per domän (flest först) ---\n",
      "geodata.scb.se                      18369\n",
      "www.scb.se                          2931\n",
      "kommunsiffror.scb.se                1444\n",
      "www.statistikdatabasen.scb.se       472\n",
      "share.scb.se                        392\n",
      "www.ssd.scb.se                      95\n",
      "www.sverigeisiffror.scb.se          69\n",
      "scb.se                              36\n",
      "www.gis.scb.se                      13\n",
      "regina.scb.se                       8\n",
      "www.h.scb.se                        7\n",
      "www.myndighetsregistret.scb.se      5\n",
      "www.pubkat.scb.se                   3\n",
      "www.scb.se:80                       3\n",
      "w41.scb.se                          2\n",
      "www.orestat.scb.se                  2\n",
      "www.h5.scb.se                       2\n",
      "myndighetsregistret.scb.se          2\n",
      "api.scb.se                          2\n",
      "w42.ssd.scb.se                      2\n",
      "www.h6.scb.se                       1\n",
      "www.rennaringsstatistik.scb.se      1\n",
      "sverigeisiffror.scb.se              1\n",
      "w36.scb.se                          1\n",
      "sni2007.scb.se                      1\n",
      "www.sni2007.scb.se                  1\n",
      "www.intrastat.scb.se                1\n",
      "www..scb.se                         1\n",
      "www.pxweb.scb.se                    1\n",
      "ssd.scb.se                          1\n",
      "www.pc-axis.scb.se                  1\n",
      "statistikdatabasen.scb.se           1\n",
      "\n",
      "Antal domäner: 32\n",
      "\n",
      "Laddade tidigare status: 3166\n",
      "\n",
      "--- Sammanfattning ---\n",
      "Totalt unika URL:         23871\n",
      "Redan testade:             3166\n",
      "Kvar att testa:            20705\n",
      "Antal domäner:             32\n",
      "\n",
      "Topp 10 domäner:\n",
      "geodata.scb.se                   18369\n",
      "www.scb.se                        2931\n",
      "kommunsiffror.scb.se              1444\n",
      "www.statistikdatabasen.scb.se      472\n",
      "share.scb.se                       392\n",
      "www.ssd.scb.se                      95\n",
      "www.sverigeisiffror.scb.se          69\n",
      "scb.se                              36\n",
      "www.gis.scb.se                      13\n",
      "regina.scb.se                        8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time \n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "CHECKPOINT_LINKS  = \"results/_checkpoint_exturlusage.csv\"\n",
    "CHECKPOINT_STATUS = \"results/_checkpoint_status.csv\"\n",
    "OUT_ALL           = \"results/all_links.csv\"\n",
    "OUT_DEAD          = \"results/dead_links.csv\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# LADDA LÄNKAR\n",
    "# -------------------------------------------------\n",
    "df = pd.read_csv(CHECKPOINT_LINKS)\n",
    "\n",
    "print(\"Alla rader i checkpoint:\", len(df))\n",
    "\n",
    "# Ta unika URL:er\n",
    "urls = df[\"url\"].dropna().unique().tolist()\n",
    "total = len(urls)\n",
    "print(f\"Totalt unika URL: {total}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# LÄNKAR PER WIKIPEDIA-SPRÅK\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Försök hitta vilken kolumn som innehåller språkkoden\n",
    "possible_lang_cols = [\"lang\", \"language\", \"wiki\", \"wiki_lang\", \"language_code\"]\n",
    "lang_col = None\n",
    "\n",
    "for c in possible_lang_cols:\n",
    "    if c in df.columns:\n",
    "        lang_col = c\n",
    "        break\n",
    "\n",
    "if lang_col is None:\n",
    "    print(\"⚠️ Hittar ingen språk-kolumn i datan! Inga språk-stats kan visas.\")\n",
    "else:\n",
    "    print(f\"\\nSpråk-kolumn hittad: '{lang_col}'\")\n",
    "\n",
    "    # Totalt antal länkar per språk (inkl. dubbletter)\n",
    "    print(\"\\n--- Statistik per språk (alla rader) ---\")\n",
    "    lang_counts_all = df[lang_col].value_counts()\n",
    "    for lang, cnt in lang_counts_all.items():\n",
    "        print(f\"{lang:5} {cnt}\")\n",
    "\n",
    "    # Antal UNIKA URLer per språk\n",
    "    print(\"\\n--- Statistik per språk (unika URL:er) ---\")\n",
    "    df_unique = df.drop_duplicates(subset=[\"url\"])\n",
    "    lang_counts_unique = df_unique[lang_col].value_counts()\n",
    "    for lang, cnt in lang_counts_unique.items():\n",
    "        print(f\"{lang:5} {cnt}\")\n",
    "\n",
    "    print(\"\\nAntal språk:\", len(lang_counts_all))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# DOMÄNSTATISTIK\n",
    "# -------------------------------------------------\n",
    "domains = [urlparse(u).netloc.lower() for u in urls]\n",
    "domain_counts = pd.Series(domains).value_counts()\n",
    "\n",
    "print(\"\\n--- Statistik per domän (flest först) ---\")\n",
    "for dom, cnt in domain_counts.items():\n",
    "    print(f\"{dom:35} {cnt}\")\n",
    "\n",
    "print(\"\\nAntal domäner:\", len(domain_counts))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# LADDA PREVIOUS STATUS (resume)\n",
    "# -------------------------------------------------\n",
    "status_map = {}\n",
    "if os.path.exists(CHECKPOINT_STATUS):\n",
    "    prev = pd.read_csv(CHECKPOINT_STATUS)\n",
    "    status_map = dict(zip(prev[\"url\"], prev[\"status\"]))\n",
    "    print(\"\\nLaddade tidigare status:\", len(status_map))\n",
    "\n",
    "# Filtrera bort redan testade länkar\n",
    "remaining = [u for u in urls if u not in status_map]\n",
    "\n",
    "print(\"\\n--- Sammanfattning ---\")\n",
    "print(f\"Totalt unika URL:         {total}\")\n",
    "print(f\"Redan testade:             {len(status_map)}\")\n",
    "print(f\"Kvar att testa:            {len(remaining)}\")\n",
    "print(f\"Antal domäner:             {len(domain_counts)}\")\n",
    "\n",
    "# Visa topp 10 domäner\n",
    "print(\"\\nTopp 10 domäner:\")\n",
    "print(domain_counts.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c753d74-0212-4e97-beac-ee66e461ac2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b73f16d7-0c27-4dcc-8a9d-b52f50afc2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Fetch exturlusage entries for one lang/domain\n",
    "# -----------------------------------------------------------\n",
    "def fetch_exturlusage(lang, domain):\n",
    "    base = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"exturlusage\",\n",
    "        \"euquery\": domain,\n",
    "        \"eulimit\": \"max\"\n",
    "    }\n",
    "    while True:\n",
    "        r = session.get(base, params=params, timeout=10)\n",
    "        data = r.json()\n",
    "        for item in data.get(\"query\", {}).get(\"exturlusage\", []):\n",
    "            yield {\n",
    "                \"lang\": lang,\n",
    "                \"page_title\": item.get(\"title\"),\n",
    "                \"url\": item.get(\"url\"),\n",
    "                \"wiki_link\": f\"https://{lang}.wikipedia.org/wiki/{item.get('title').replace(' ', '_')}\"\n",
    "            }\n",
    "\n",
    "        if \"continue\" not in data:\n",
    "            break\n",
    "        params.update(data[\"continue\"])\n",
    "        time.sleep(0.3)\n",
    "# -----------------------------------------------------------\n",
    "# Modified main() with resume support\n",
    "# -----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d225559-ee19-4e9e-a31d-c4f8215badbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totalt unika URL: 23871\n",
      "Laddade tidigare status: 3166\n",
      "Återstår att testa: 20705\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccc43ec7ae8d420eacdd5111b9d7bedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Kollar SCB-länkar:   0%|          | 0/20705 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COOLDOWN] lat=0.05s err=100.00% → väntar 60s\n",
      "[COOLDOWN] lat=0.11s err=100.00% → väntar 120s\n",
      "[COOLDOWN] lat=0.02s err=100.00% → väntar 240s\n",
      "[COOLDOWN] lat=0.04s err=100.00% → väntar 480s\n",
      "[COOLDOWN] lat=0.03s err=100.00% → väntar 960s\n",
      "[COOLDOWN] lat=0.02s err=100.00% → väntar 1800s\n",
      "[COOLDOWN] lat=0.05s err=100.00% → väntar 1800s\n",
      "[COOLDOWN] lat=0.27s err=100.00% → väntar 1800s\n",
      "[COOLDOWN] lat=0.02s err=100.00% → väntar 1800s\n",
      "[COOLDOWN] lat=0.12s err=100.00% → väntar 1800s\n",
      "[COOLDOWN] lat=0.18s err=100.00% → väntar 1800s\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "#  SCB LINK CHECKER — Notebook Optimized\n",
    "#  Stabil, resume, progressbar, adaptiv hastighet\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# KONFIG\n",
    "# -------------------------\n",
    "CHECKPOINT_LINKS  = \"results/_checkpoint_exturlusage.csv\"\n",
    "CHECKPOINT_STATUS = \"results/_checkpoint_status.csv\"\n",
    "OUT_ALL           = \"results/all_links.csv\"\n",
    "OUT_DEAD          = \"results/dead_links.csv\"\n",
    "\n",
    "# Adaptiv request-hantering\n",
    "REQUEST_TIMEOUT   = 1.2       # SCB klarar inte kortare timeout\n",
    "PAUSE_MIN         = 0.10\n",
    "PAUSE_MAX         = 1.50\n",
    "ADAPT_UP          = 0.10\n",
    "ADAPT_DOWN        = 0.04\n",
    "ROLLING_N         = 150\n",
    "SAVE_EVERY        = 200        # checkpoint oftare\n",
    "ERR_RATE_THRESHOLD = 0.50\n",
    "LATENCY_THRESHOLD  = 1.5\n",
    "COOLDOWN_BASE      = 60\n",
    "COOLDOWN_MAX       = 1800\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"SCB-LinkTest/1.0 (contact: salgo60@msn.com)\"\n",
    "})\n",
    "\n",
    "SCB_SOFT_404 = [\n",
    "    \"sidan kunde inte hittas\",\n",
    "    \"vi vill gärna hjälpa dig vidare\",\n",
    "    \"snabba fakta om sverige\",\n",
    "    \"ta dig vidare till\",\n",
    "]\n",
    "\n",
    "def detect_scb_soft_404(url, response):\n",
    "    text = response.text.lower()\n",
    "    if any(sig in text for sig in SCB_SOFT_404):\n",
    "        return \"SOFT_404\"\n",
    "    if response.status_code == 404:\n",
    "        return \"HARD_404\"\n",
    "    return 200\n",
    "\n",
    "def check_status(url):\n",
    "    t0 = time.time()\n",
    "    try:\n",
    "        r = session.get(url, timeout=REQUEST_TIMEOUT, allow_redirects=True, stream=True)\n",
    "        code = r.status_code\n",
    "\n",
    "        if \"scb.se\" in url.lower():\n",
    "            return detect_scb_soft_404(url, r), time.time() - t0\n",
    "\n",
    "        return code, time.time() - t0\n",
    "\n",
    "    except Exception:\n",
    "        return \"ERROR\", time.time() - t0\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# LADDA LÄNKAR\n",
    "# -------------------------\n",
    "if not os.path.exists(CHECKPOINT_LINKS):\n",
    "    raise FileNotFoundError(\"Hittar inte länkar i results/_checkpoint_exturlusage.csv – kör exturlusage först!\")\n",
    "\n",
    "df = pd.read_csv(CHECKPOINT_LINKS)\n",
    "urls = df[\"url\"].dropna().unique().tolist()\n",
    "print(\"Totalt unika URL:\", len(urls))\n",
    "\n",
    "# -------------------------\n",
    "# LADDA STATUS-CHECKPOINT\n",
    "# -------------------------\n",
    "status_map = {}\n",
    "if os.path.exists(CHECKPOINT_STATUS):\n",
    "    prev = pd.read_csv(CHECKPOINT_STATUS)\n",
    "    status_map = dict(zip(prev[\"url\"], prev[\"status\"]))\n",
    "    print(\"Laddade tidigare status:\", len(status_map))\n",
    "\n",
    "remaining = [u for u in urls if u not in status_map]\n",
    "print(\"Återstår att testa:\", len(remaining))\n",
    "\n",
    "# -------------------------\n",
    "# ADAPTIV LOOP\n",
    "# -------------------------\n",
    "pause = 0.30\n",
    "recent = []\n",
    "cooldown_factor = 0\n",
    "\n",
    "pbar = tqdm(total=len(remaining), desc=\"Kollar SCB-länkar\")\n",
    "\n",
    "for url in remaining:\n",
    "\n",
    "    status, latency = check_status(url)\n",
    "    status_map[url] = status\n",
    "\n",
    "    # Rolling window\n",
    "    is_err = 1 if status not in (200,) else 0\n",
    "    recent.append((latency, is_err))\n",
    "    if len(recent) > ROLLING_N:\n",
    "        recent = recent[-ROLLING_N:]\n",
    "\n",
    "    latency_avg = sum(t for t,e in recent) / len(recent)\n",
    "    err_rate = sum(e for _,e in recent) / len(recent)\n",
    "\n",
    "    # Cooldown vid dålig period\n",
    "    if latency_avg > LATENCY_THRESHOLD or err_rate > ERR_RATE_THRESHOLD:\n",
    "        cooldown_factor = min(cooldown_factor + 1, 6)\n",
    "        cooldown = min(COOLDOWN_BASE * (2 ** (cooldown_factor - 1)), COOLDOWN_MAX)\n",
    "\n",
    "        print(f\"[COOLDOWN] lat={latency_avg:.2f}s err={err_rate:.2%} → väntar {cooldown}s\")\n",
    "        pd.DataFrame([{\"url\":u,\"status\":s} for u,s in status_map.items()]).to_csv(CHECKPOINT_STATUS, index=False)\n",
    "\n",
    "        time.sleep(cooldown)\n",
    "        recent = []\n",
    "        pause = PAUSE_MAX\n",
    "    else:\n",
    "        if cooldown_factor > 0:\n",
    "            cooldown_factor = max(0, cooldown_factor - 1)\n",
    "\n",
    "    # Adaptiv paus\n",
    "    if latency_avg > 0.9:\n",
    "        pause = min(PAUSE_MAX, pause + ADAPT_UP)\n",
    "    elif latency_avg < 0.5:\n",
    "        pause = max(PAUSE_MIN, pause - ADAPT_DOWN)\n",
    "\n",
    "    jitter = random.uniform(-0.04, 0.12)\n",
    "    actual_sleep = max(PAUSE_MIN, min(PAUSE_MAX, pause + jitter))\n",
    "\n",
    "    pbar.set_postfix({\n",
    "        \"lat\": f\"{latency_avg:.2f}s\",\n",
    "        \"pause\": f\"{actual_sleep:.2f}s\",\n",
    "        \"err\": f\"{err_rate:.2%}\",\n",
    "        \"status\": status\n",
    "    })\n",
    "\n",
    "    time.sleep(actual_sleep)\n",
    "    pbar.update(1)\n",
    "\n",
    "    # Checkpoint\n",
    "    if pbar.n % SAVE_EVERY == 0:\n",
    "        pd.DataFrame([{\"url\":u,\"status\":s} for u,s in status_map.items()]).to_csv(CHECKPOINT_STATUS, index=False)\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "# -------------------------\n",
    "# SPARA RESULTAT\n",
    "# -------------------------\n",
    "pd.DataFrame([{\"url\":u,\"status\":s} for u,s in status_map.items()]).to_csv(CHECKPOINT_STATUS, index=False)\n",
    "\n",
    "df[\"status\"] = df[\"url\"].map(status_map)\n",
    "df.to_csv(OUT_ALL, index=False)\n",
    "\n",
    "dead = df[df[\"status\"].isin([\"HARD_404\", \"SOFT_404\", \"ERROR\"])]\n",
    "dead.to_csv(OUT_DEAD, index=False)\n",
    "\n",
    "print(\"KLART!\")\n",
    "print(\"All links:\", OUT_ALL)\n",
    "print(\"Dead links:\", OUT_DEAD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8cd01f-ad22-4286-ac66-6aadd558147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap + CSV/HTML export + bar chart (Notebook)\n",
    "import os\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Helper for interactive display (optional)\n",
    "try:\n",
    "    from ace_tools import display_dataframe_to_user\n",
    "    have_display = True\n",
    "except Exception:\n",
    "    have_display = False\n",
    "\n",
    "# Paths - ändra om din fil ligger annorlunda\n",
    "CHECKPOINT_LINKS = \"results/_checkpoint_exturlusage.csv\"\n",
    "OUT_CSV = \"results/heatmap_counts2.csv\"\n",
    "OUT_HTML = \"results/heatmap_counts2.html\"\n",
    "OUT_LOG = \"results/domain_lang_summary.csv\"\n",
    "\n",
    "# Kontrollera fil\n",
    "if not os.path.exists(CHECKPOINT_LINKS):\n",
    "    raise FileNotFoundError(f\"Input file not found: {CHECKPOINT_LINKS}. Flytta filen eller uppdatera CHECKPOINT_LINKS.\")\n",
    "\n",
    "# Läs in\n",
    "df = pd.read_csv(CHECKPOINT_LINKS)\n",
    "\n",
    "# Försök hitta språk-kolumn\n",
    "possible_lang_cols = [\"lang\", \"language\", \"wiki\", \"wiki_lang\", \"language_code\"]\n",
    "lang_col = next((c for c in possible_lang_cols if c in df.columns), None)\n",
    "if lang_col is None:\n",
    "    for c in df.columns:\n",
    "        if c.lower() in possible_lang_cols:\n",
    "            lang_col = c\n",
    "            break\n",
    "if lang_col is None:\n",
    "    raise ValueError(\"Hittar ingen språk-kolumn i CSV. Förväntade någon av: \" + \", \".join(possible_lang_cols))\n",
    "\n",
    "# Extrahera domän\n",
    "def extract_domain(u):\n",
    "    try:\n",
    "        return urlparse(u).netloc.lower()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "df['domain'] = df['url'].fillna('').apply(extract_domain)\n",
    "df[lang_col] = df[lang_col].astype(str).str.strip()\n",
    "\n",
    "# Pivot: språk x domän\n",
    "pivot = df.pivot_table(index=lang_col, columns='domain', values='url', aggfunc='count', fill_value=0)\n",
    "lang_totals = pivot.sum(axis=1).sort_values(ascending=False)\n",
    "pivot = pivot.reindex(lang_totals.index)\n",
    "\n",
    "# Totals per domain\n",
    "domain_totals = pivot.sum(axis=0).sort_values(ascending=False)\n",
    "\n",
    "# Save outputs\n",
    "pivot.to_csv(OUT_CSV)\n",
    "pivot.to_html(OUT_HTML)\n",
    "summary = pivot.reset_index().melt(id_vars=[lang_col], var_name='domain', value_name='count')\n",
    "summary = summary.sort_values(['count'], ascending=False)\n",
    "summary.to_csv(OUT_LOG, index=False)\n",
    "\n",
    "# Present a preview\n",
    "preview = pivot.copy()\n",
    "if preview.shape[1] > 40:\n",
    "    preview = preview.iloc[:, :40]\n",
    "if have_display:\n",
    "    display_dataframe_to_user(\"Language x Domain counts (preview)\", preview.reset_index())\n",
    "else:\n",
    "    print(\"Preview (truncated to first 40 domains):\")\n",
    "    display(preview.reset_index().head(40))\n",
    "\n",
    "# --- Heatmap (top domains) ---\n",
    "TOP_DOMAINS = 30\n",
    "top_domains = domain_totals.head(TOP_DOMAINS).index.tolist()\n",
    "plot_pivot = pivot[top_domains]\n",
    "\n",
    "plt.figure(figsize=(max(8, len(top_domains)*0.35), max(6, len(plot_pivot.index)*0.25)))\n",
    "plt.imshow(plot_pivot.values, aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='Count')\n",
    "plt.xticks(ticks=np.arange(len(top_domains)), labels=top_domains, rotation=90, fontsize=8)\n",
    "plt.yticks(ticks=np.arange(len(plot_pivot.index)), labels=plot_pivot.index, fontsize=8)\n",
    "plt.title('Heatmap: links per language × top domains')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Bar chart of top domains overall ---\n",
    "top20 = domain_totals.head(20)\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(range(len(top20)), top20.values)\n",
    "plt.xticks(range(len(top20)), top20.index, rotation=90, fontsize=8)\n",
    "plt.ylabel('Total links')\n",
    "plt.title('Top 20 domains by total links')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Filvägar\n",
    "print(\"\\nSaved files:\")\n",
    "print(f\"- Full pivot CSV: {OUT_CSV}\")\n",
    "print(f\"- Full pivot HTML: {OUT_HTML}\")\n",
    "print(f\"- Long-form summary CSV: {OUT_LOG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5e942-32c4-42a4-8ce6-3bbfff2a2e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "st.title(\"SCB Link Audit — Wikipedia\")\n",
    "\n",
    "st.write(\"Visualisering av länkröta relaterad till scb.se på Wikipedia.\")\n",
    "\n",
    "df_all = pd.read_csv(\"results/all_links.csv\")\n",
    "df_dead = pd.read_csv(\"results/dead_links.csv\")\n",
    "df_art = pd.read_csv(\"results/stats_per_article.csv\")\n",
    "df_path = pd.read_csv(\"results/stats_per_domain_path.csv\")\n",
    "\n",
    "st.header(\"1. Översikt\")\n",
    "col1, col2, col3 = st.columns(3)\n",
    "\n",
    "col1.metric(\"Totala länkar\", len(df_all))\n",
    "col2.metric(\"Döda länkar\", len(df_dead))\n",
    "col3.metric(\"Döda (%)\", f\"{100 * len(df_dead)/len(df_all):.2f}%\")\n",
    "\n",
    "st.header(\"2. Status-fördelning\")\n",
    "st.bar_chart(df_all[\"status\"].value_counts())\n",
    "\n",
    "st.header(\"3. Artiklar med flest trasiga länkar\")\n",
    "bad = df_art[df_art[\"dead_links\"] > 0].sort_values(\"dead_links\", ascending=False)\n",
    "st.dataframe(bad)\n",
    "\n",
    "st.header(\"4. SCB paths där flest länkar gått sönder\")\n",
    "st.dataframe(df_path.sort_values(\"dead_ratio\", ascending=False))\n",
    "\n",
    "st.header(\"5. Alla länkar\")\n",
    "st.dataframe(df_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75daac4d-42af-48f4-a602-ba1b6c7e106c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # End timer and calculate duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time# Bygg audit-lager för den här etappen\n",
    "\n",
    "# Print current date and total time\n",
    "print(\"Date:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "print(\"Total time elapsed: {:02.0f} minutes {:05.2f} seconds\".format(minutes, seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c251a2-4190-4a21-aba7-eeff4114f68a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
