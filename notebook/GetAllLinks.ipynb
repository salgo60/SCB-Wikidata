{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00135966-9848-4ffa-82d2-054a96f066cf",
   "metadata": {},
   "source": [
    "* [#4](https://github.com/salgo60/SCB-Wikidata/issues/4)\n",
    "* denna [GetAllLinks.ipynb](https://github.com/salgo60/SCB-Wikidata/blob/main/notebook/GetSCBLinks.ipynb)\n",
    "\n",
    "Tanken är att kolla alla Wikipedia domäner för alla svenska myndigheter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8fc5e84-78d7-47d6-9247-3d18066823e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2025-11-18 13:34:55\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "timestamp = now.timestamp()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Start:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6a6eb6b-b9a9-4d40-a4cc-73045447a27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_language_codes(file_path): \n",
    "    print(f\"[DEBUG] Reading language codes from: {file_path}\") \n",
    "    df = pd.read_csv(file_path) \n",
    "    lang_codes = df['Language Code'].dropna().unique().tolist() \n",
    "    print(f\"[DEBUG] Found {len(lang_codes)} language codes.\") \n",
    "    return lang_codes \n",
    "    \n",
    "def read_domains(file_path): \n",
    "    print(f\"[DEBUG] Reading domains from: {file_path}\") \n",
    "    df = pd.read_csv(file_path, header=None, names=['domain']) \n",
    "    domains_list = df['domain'].dropna().unique().tolist() \n",
    "    print(f\"[DEBUG] Found {len(domains_list)} domains.\") \n",
    "    return domains_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "464fdc56-3026-4b49-ac51-5d30296c19ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Fetching sitematrix…\n",
      "OK! Keys: dict_keys(['sitematrix'])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def fetch_sitematrix():\n",
    "    url = \"https://meta.wikimedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"sitematrix\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    headers = {\n",
    "        \"User-Agent\": \"salgo60-language-fetcher/1.0 (salgo60@msn.com)\"\n",
    "    }\n",
    "\n",
    "    print(\"[DEBUG] Fetching sitematrix…\")\n",
    "    r = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "    # Debug om det fortfarande inte är JSON\n",
    "    if \"application/json\" not in r.headers.get(\"Content-Type\", \"\"):\n",
    "        print(\"[ERROR] Server returned non-JSON response:\")\n",
    "        print(r.text[:500])\n",
    "        raise ValueError(\"Server returned non-JSON\")\n",
    "\n",
    "    return r.json()\n",
    "\n",
    "data = fetch_sitematrix()\n",
    "print(\"OK! Keys:\", data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "046eda9d-8ea8-47c5-a1ea-1f6b21bd60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def debug_sitematrix():\n",
    "    url = \"https://meta.wikimedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"sitematrix\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    headers = {\n",
    "        \"User-Agent\": \"salgo60-language-fetcher/1.0 (https://github.com/salgo60; contact=salgo60@msn.com)\"\n",
    "    }\n",
    "\n",
    "    print(\"[DEBUG] Requesting…\")\n",
    "    r = requests.get(url, params=params, headers=headers)\n",
    "\n",
    "    print(\"[DEBUG] Status:\", r.status_code)\n",
    "    print(\"[DEBUG] Content-Type:\", r.headers.get(\"Content-Type\"))\n",
    "    print(\"\\n---------- RESPONSE START ----------\\n\")\n",
    "    print(r.text[:5000])  # skriv ut upp till 5000 tecken\n",
    "    print(\"\\n---------- RESPONSE END ----------\\n\")\n",
    "\n",
    "#debug_sitematrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2da705ab-cdf3-41a2-a517-646def190f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /Users/salgo/Documents/GitHub/SCB-Wikidata/notebook\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current Working Directory:\", current_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a0aeecd-2087-483c-a054-dcef0e05bef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"salgo60-language-fetcher/2.0 (https://github.com/salgo60)\"\n",
    "}\n",
    "\n",
    "\n",
    "def fetch_sitematrix():\n",
    "    \"\"\"Hämtar sitematrix med korrekt User-Agent.\"\"\"\n",
    "    url = \"https://meta.wikimedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"sitematrix\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "\n",
    "    r = requests.get(url, params=params, headers=HEADERS)\n",
    "\n",
    "    if \"application/json\" not in r.headers.get(\"Content-Type\", \"\"):\n",
    "        raise ValueError(\"Non-JSON response:\\n\" + r.text[:500])\n",
    "\n",
    "    return r.json()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20cc6073-f72f-4baa-af19-277d246dbe10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Reading language codes from: sources/wiki_versions_all.csv\n",
      "[DEBUG] Found 323 language codes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ab</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ady</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>af</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sq</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0\n",
       "0   ab\n",
       "1  ace\n",
       "2  ady\n",
       "3   af\n",
       "4   sq"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path_lang = \"sources/wiki_versions_all.csv\"\n",
    "\n",
    "lang_list = read_language_codes(file_path_lang)\n",
    "df_lang = pd.DataFrame(lang_list)\n",
    "df_lang.head() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3401caee-7087-4deb-96e3-143c3618974f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def normalize_domain(url):\n",
    "    if not isinstance(url, str) or url.strip() == \"\":\n",
    "        return None\n",
    "    \n",
    "    # Parse URL\n",
    "    p = urlparse(url)\n",
    "    \n",
    "    # Extract hostname (www.xxx.se)\n",
    "    host = p.netloc\n",
    "    \n",
    "    # Remove 'www.' prefix if you want (optional)\n",
    "    host = host.replace(\"www.\", \"\")\n",
    "    \n",
    "    return host\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce2ac358-e50d-4a4e-933d-3948b86f3f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Reading language codes from: sources/wiki_versions_all.csv\n",
      "[DEBUG] Found 323 language codes.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "# --- 1. Load language list ---\n",
    "file_path_lang = \"sources/wiki_versions_all.csv\"\n",
    "lang_list = read_language_codes(file_path_lang)       # ['sv', 'en', 'de', ...]\n",
    "df_lang = pd.DataFrame(lang_list, columns=[\"lang\"])\n",
    "\n",
    "\n",
    "# --- 2. Fetch Swedish government agencies from Wikidata ---\n",
    "def fetch_swedish_agencies():\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    query = \"\"\"\n",
    "     SELECT ?qid ?qidLabel ?website WHERE {\n",
    "      ?qid wdt:P31 wd:Q68295960 .     # instance of: Swedish government agency\n",
    "      OPTIONAL { ?qid wdt:P856 ?website }  # website\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"sv,en\". }\n",
    "    } order by ?qidLabel\n",
    "\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "\n",
    "    rows = []\n",
    "    for r in results[\"results\"][\"bindings\"]:\n",
    "        rows.append({\n",
    "            \"qid\": r[\"qid\"][\"value\"].split(\"/\")[-1],\n",
    "            \"agency_label\": r[\"qidLabel\"][\"value\"],\n",
    "            \"domain\": r.get(\"website\", {}).get(\"value\", None)\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "df_agencies = fetch_swedish_agencies()\n",
    "df_agencies[\"domain_clean\"] = df_agencies[\"domain\"].apply(normalize_domain)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6bb05ead-ca19-43af-9dde-3bb1582d3546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>agency_label</th>\n",
       "      <th>domain</th>\n",
       "      <th>domain_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q104870569</td>\n",
       "      <td>Alkoholinspektionen</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q10405389</td>\n",
       "      <td>Alkoholsortimentsnämnden</td>\n",
       "      <td>https://www.kammarkollegiet.se/om-oss/organisa...</td>\n",
       "      <td>kammarkollegiet.se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q127946640</td>\n",
       "      <td>Allmänna advokatbyråerna</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q6971047</td>\n",
       "      <td>Allmänna reklamationsnämnden</td>\n",
       "      <td>http://www.arn.se/</td>\n",
       "      <td>arn.se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q10413037</td>\n",
       "      <td>Ansvarsnämnden för djurens hälso- och sjukvård</td>\n",
       "      <td>https://www.vetansvar.se</td>\n",
       "      <td>vetansvar.se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>Q10728611</td>\n",
       "      <td>Överklagandenämnden för högskolan</td>\n",
       "      <td>https://www.onh.se</td>\n",
       "      <td>onh.se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>Q80207153</td>\n",
       "      <td>Överklagandenämnden för nämndemannauppdrag</td>\n",
       "      <td>https://www.domstol.se/overklagandenamnden-for...</td>\n",
       "      <td>domstol.se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>Q10728612</td>\n",
       "      <td>Överklagandenämnden för studiestöd</td>\n",
       "      <td>https://www.oks.se</td>\n",
       "      <td>oks.se</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>Q30696695</td>\n",
       "      <td>Överstyrelsen för yrkesutbildning</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>Q4994097</td>\n",
       "      <td>överståthållarämbetet</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>407 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            qid                                    agency_label  \\\n",
       "0    Q104870569                             Alkoholinspektionen   \n",
       "1     Q10405389                        Alkoholsortimentsnämnden   \n",
       "2    Q127946640                        Allmänna advokatbyråerna   \n",
       "3      Q6971047                    Allmänna reklamationsnämnden   \n",
       "4     Q10413037  Ansvarsnämnden för djurens hälso- och sjukvård   \n",
       "..          ...                                             ...   \n",
       "402   Q10728611               Överklagandenämnden för högskolan   \n",
       "403   Q80207153      Överklagandenämnden för nämndemannauppdrag   \n",
       "404   Q10728612              Överklagandenämnden för studiestöd   \n",
       "405   Q30696695               Överstyrelsen för yrkesutbildning   \n",
       "406    Q4994097                           överståthållarämbetet   \n",
       "\n",
       "                                                domain        domain_clean  \n",
       "0                                                 None                None  \n",
       "1    https://www.kammarkollegiet.se/om-oss/organisa...  kammarkollegiet.se  \n",
       "2                                                 None                None  \n",
       "3                                   http://www.arn.se/              arn.se  \n",
       "4                             https://www.vetansvar.se        vetansvar.se  \n",
       "..                                                 ...                 ...  \n",
       "402                                 https://www.onh.se              onh.se  \n",
       "403  https://www.domstol.se/overklagandenamnden-for...          domstol.se  \n",
       "404                                 https://www.oks.se              oks.se  \n",
       "405                                               None                None  \n",
       "406                                               None                None  \n",
       "\n",
       "[407 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_agencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52f3ae03-6acd-4a0d-874d-26a56b979b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1725f8b5-d67e-4e0b-b651-47ba5112530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"SAT-link-checker/1.0 (https://github.com/salgo60; contact: salgo60@msn.com)\"\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa679b4-35a8-400d-806b-2d727e406d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"SAT-link-checker/1.0 (https://github.com/salgo60; contact: salgo60)\"\n",
    "})\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# STEP 1 — Get all valid Wikipedia languages from Wikidata\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def get_wikipedia_languages():\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    query = \"\"\"\n",
    "    SELECT ?code WHERE {\n",
    "      ?wiki a wikibase:Site ;\n",
    "            wikibase:wikiGroup \"wikipedia\" ;\n",
    "            wikibase:language ?code .\n",
    "    }\n",
    "    ORDER BY ?code\n",
    "    \"\"\"\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    data = sparql.query().convert()\n",
    "\n",
    "    return [x[\"code\"][\"value\"] for x in data[\"results\"][\"bindings\"]]\n",
    "\n",
    "\n",
    "lang_list = get_wikipedia_languages()\n",
    "print(\"Wikipedia languages:\", len(lang_list))\n",
    "print(lang_list[:20], \"...\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# STEP 2 — Normalize domain\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def normalize_domain(url):\n",
    "    if not isinstance(url, str) or url.strip() == \"\":\n",
    "        return None\n",
    "    p = urlparse(url)\n",
    "    host = p.netloc.replace(\"www.\", \"\")\n",
    "    return host.split(\"/\")[0]\n",
    "\n",
    "df_agencies[\"domain_clean\"] = df_agencies[\"domain\"].apply(normalize_domain)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# STEP 3 — Test if a Wikipedia language works (API endpoint)\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def is_valid_wiki(lang):\n",
    "    url = f\"https://{lang}.wikipedia.org/w/api.php?action=query&meta=siteinfo&format=json\"\n",
    "    try:\n",
    "        r = session.get(url, timeout=5)\n",
    "        if r.status_code != 200:\n",
    "            return False\n",
    "        data = r.json()\n",
    "        return \"query\" in data\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "valid_langs = [lang for lang in lang_list if is_valid_wiki(lang)]\n",
    "\n",
    "print(\"Valid Wikipedia languages:\", len(valid_langs))\n",
    "print(valid_langs[:25], \"...\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# STEP 4 — Fetch exturlusage entries for one lang/domain\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def fetch_exturlusage(lang, domain):\n",
    "    base = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"exturlusage\",\n",
    "        \"euquery\": domain,\n",
    "        \"eulimit\": \"max\"\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            r = session.get(base, params=params, timeout=10)\n",
    "        except Exception as e:\n",
    "            print(f\"[ERROR] Request failed for {lang}: {e}\")\n",
    "            return\n",
    "\n",
    "        if r.status_code == 403:\n",
    "            print(f\"[403] Forbidden for {lang} / {domain}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            data = r.json()\n",
    "        except ValueError:\n",
    "            print(f\"[WARN] Non-JSON response for {lang} / {domain}\")\n",
    "            return\n",
    "\n",
    "        for item in data.get(\"query\", {}).get(\"exturlusage\", []):\n",
    "            yield {\n",
    "                \"lang\": lang,\n",
    "                \"page_title\": item.get(\"title\"),\n",
    "                \"url\": item.get(\"url\"),\n",
    "                \"wiki_link\": f\"https://{lang}.wikipedia.org/wiki/{item.get('title').replace(' ', '_')}\"\n",
    "            }\n",
    "\n",
    "        if \"continue\" not in data:\n",
    "            break\n",
    "\n",
    "        params.update(data[\"continue\"])\n",
    "        time.sleep(0.3)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# STEP 5 — Loop agencies + languages\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def get_all_outlinks(df_agencies, lang_list):\n",
    "    rows = []\n",
    "\n",
    "    for _, row in df_agencies.iterrows():\n",
    "        qid = row[\"qid\"]\n",
    "        label = row[\"agency_label\"]\n",
    "        domain_clean = row[\"domain_clean\"]\n",
    "\n",
    "        if not isinstance(domain_clean, str) or domain_clean.strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nChecking {qid} / {label} / {domain_clean}\")\n",
    "\n",
    "        for lang in lang_list:\n",
    "            try:\n",
    "                for hit in fetch_exturlusage(lang, domain_clean):\n",
    "                    rows.append({\n",
    "                        \"qid\": qid,\n",
    "                        \"agency_label\": label,\n",
    "                        \"domain\": domain_clean,\n",
    "                        \"lang\": hit[\"lang\"],\n",
    "                        \"page_title\": hit[\"page_title\"],\n",
    "                        \"wiki_link\": hit[\"wiki_link\"],\n",
    "                        \"url\": hit[\"url\"]\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] {domain_clean} @ {lang}: {e}\")\n",
    "                continue\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# STEP 6 — RUN\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "df_outlinks = get_all_outlinks(df_agencies, valid_langs)\n",
    "df_outlinks.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4ba8b6-cc6f-4363-a9df-627c468f0897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
