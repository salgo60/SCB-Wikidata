{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74da516d-bdd3-489b-a98d-e6fa71145599",
   "metadata": {},
   "source": [
    "* [#4](https://github.com/salgo60/SCB-Wikidata/issues/4)\n",
    "* denna [SCB4_linkroot.ipynb](https://github.com/salgo60/SCB-Wikidata/blob/main/notebook/SCB4_linkroot.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d311f58-d8a4-42f9-aa33-819e7988c23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2025-11-16 02:52:25\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "timestamp = now.timestamp()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Start:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ce80ab-900c-4569-9baf-9f593c028ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log(level, msg):\n",
    "    ts = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(f\"[{ts}] [{level}] {msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53643e95-b697-4cf4-a62c-262f756ea52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_language_codes(file_path): \n",
    "    print(f\"[DEBUG] Reading language codes from: {file_path}\") \n",
    "    df = pd.read_csv(file_path) \n",
    "    lang_codes = df['Language Code'].dropna().unique().tolist() \n",
    "    print(f\"[DEBUG] Found {len(lang_codes)} language codes.\") \n",
    "    return lang_codes \n",
    "    \n",
    "def read_domains(file_path): \n",
    "    print(f\"[DEBUG] Reading domains from: {file_path}\") \n",
    "    df = pd.read_csv(file_path, header=None, names=['domain']) \n",
    "    domains_list = df['domain'].dropna().unique().tolist() \n",
    "    print(f\"[DEBUG] Found {len(domains_list)} domains.\") \n",
    "    return domains_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd808572-a6a2-4b6c-ac33-4463461076f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b73f16d7-0c27-4dcc-8a9d-b52f50afc2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Fetch exturlusage entries for one lang/domain\n",
    "# -----------------------------------------------------------\n",
    "def fetch_exturlusage(lang, domain):\n",
    "    base = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"exturlusage\",\n",
    "        \"euquery\": domain,\n",
    "        \"eulimit\": \"max\"\n",
    "    }\n",
    "    while True:\n",
    "        r = session.get(base, params=params, timeout=10)\n",
    "        data = r.json()\n",
    "        for item in data.get(\"query\", {}).get(\"exturlusage\", []):\n",
    "            yield {\n",
    "                \"lang\": lang,\n",
    "                \"page_title\": item.get(\"title\"),\n",
    "                \"url\": item.get(\"url\"),\n",
    "                \"wiki_link\": f\"https://{lang}.wikipedia.org/wiki/{item.get('title').replace(' ', '_')}\"\n",
    "            }\n",
    "\n",
    "        if \"continue\" not in data:\n",
    "            break\n",
    "        params.update(data[\"continue\"])\n",
    "        time.sleep(0.3)\n",
    "# -----------------------------------------------------------\n",
    "# Modified main() with resume support\n",
    "# -----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d225559-ee19-4e9e-a31d-c4f8215badbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totalt unika URL: 23871\n",
      "Laddade tidigare status: 1603\n",
      "Återstår att testa: 22268\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf92da0415744becb5dcf962606f0e4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Kollar SCB-länkar:   0%|          | 0/22268 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[COOLDOWN] lat=0.1s err=100.00% => väntar 60s\n",
      "[COOLDOWN] lat=0.1s err=100.00% => väntar 120s\n",
      "[COOLDOWN] lat=0.1s err=50.00% => väntar 120s\n",
      "[COOLDOWN] lat=0.1s err=100.00% => väntar 240s\n",
      "[COOLDOWN] lat=1.4s err=42.86% => väntar 60s\n",
      "[COOLDOWN] lat=0.0s err=100.00% => väntar 120s\n",
      "[COOLDOWN] lat=0.0s err=100.00% => väntar 240s\n",
      "[COOLDOWN] lat=0.1s err=100.00% => väntar 480s\n",
      "[COOLDOWN] lat=0.2s err=100.00% => väntar 960s\n",
      "[COOLDOWN] lat=0.1s err=100.00% => väntar 1800s\n",
      "[COOLDOWN] lat=0.8s err=100.00% => väntar 1800s\n",
      "[COOLDOWN] lat=0.2s err=45.45% => väntar 60s\n",
      "[COOLDOWN] lat=0.0s err=100.00% => väntar 120s\n",
      "[COOLDOWN] lat=0.0s err=100.00% => väntar 240s\n",
      "[COOLDOWN] lat=0.0s err=100.00% => väntar 480s\n",
      "[COOLDOWN] lat=0.1s err=100.00% => väntar 960s\n",
      "[COOLDOWN] lat=0.1s err=100.00% => väntar 1800s\n",
      "[COOLDOWN] lat=0.1s err=100.00% => väntar 1800s\n",
      "[COOLDOWN] lat=0.1s err=100.00% => väntar 1800s\n",
      "[COOLDOWN] lat=0.3s err=50.00% => väntar 1800s\n",
      "[COOLDOWN] lat=0.1s err=100.00% => väntar 1800s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 124\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[COOLDOWN] lat=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatency_avg\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms err=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr_rate\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m => väntar \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcooldown_seconds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    120\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m    121\u001b[0m     [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: u, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: s} \u001b[38;5;28;01mfor\u001b[39;00m u, s \u001b[38;5;129;01min\u001b[39;00m status_map\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    122\u001b[0m )\u001b[38;5;241m.\u001b[39mto_csv(CHECKPOINT_STATUS, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 124\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcooldown_seconds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m recent\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m    126\u001b[0m pause \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(pause \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.2\u001b[39m, PAUSE_MAX)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Klistra in i en Notebook-cell och kör.\n",
    "# Kräver: requests, tqdm, pandas\n",
    "# !pip install requests tqdm pandas\n",
    "\n",
    "import time\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib.parse import urlparse\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG — justera vid behov\n",
    "# -------------------------\n",
    "CONFIG = {\n",
    "    \"wiki_versions_file\": \"sources/wiki_versions.csv\",\n",
    "    \"domains_file\": \"sources/domains.csv\",\n",
    "    \"checkpoint_links\": \"results/_checkpoint_exturlusage.csv\",\n",
    "    \"checkpoint_status\": \"results/_checkpoint_status.csv\",\n",
    "    \"out_all_links\": \"results/all_links.csv\",\n",
    "    \"out_dead_links\": \"results/dead_links.csv\",\n",
    "    \"max_workers_cap\": 1,        # hårt tak\n",
    "    \"initial_concurrency\": 80,    # startvärde för adaptiv concurrency\n",
    "    \"min_concurrency\": 5,\n",
    "    \"monitor_interval\": 5.0,      # sek mellan monitor-justeringar\n",
    "    \"rolling_window\": 200,        # hur många senaste requests som används för latency/err rate\n",
    "    \"target_avg_latency\": 1.0,    # mål (s) - om avg>target -> minska concurrency\n",
    "    \"increase_step\": 5,           # öka concurrency med detta om allt är bra\n",
    "    \"decrease_step\": 10,          # minska concurrency med detta om långsamt\n",
    "    \"per_domain_backoff_base\": 10, # sekunder baseline för exponential backoff\n",
    "    \"per_domain_fail_threshold\": 3,# antal på varandra följande fel innan backoff eskalerar\n",
    "    \"status_save_every\": 500,     # spara checkpoints varje N färdiga requests\n",
    "    \"request_timeout\": 0.7,       # timeout för requests\n",
    "    \"use_get_stream\": True,       # om True: använd GET stream=True istället för HEAD\n",
    "    \"max_retries\": 1,             # simpla retries per request (on transient errors)\n",
    "}\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Session & helpers\n",
    "# -------------------------\n",
    "session = requests.Session()\n",
    "session.headers.update({\"User-Agent\": \"SCB-LinkAudit/1.0 (https://www.scb.se/)\"})\n",
    "\n",
    "SCB_SOFT_404_SIGNALS = [\n",
    "    \"sidan kunde inte hittas\",\n",
    "    \"vi vill gärna hjälpa dig vidare\",\n",
    "    \"snabba fakta om sverige\",\n",
    "    \"ta dig vidare till\"\n",
    "]\n",
    "\n",
    "def detect_scb_soft_404(url):\n",
    "    try:\n",
    "        r = session.get(url, timeout=6)\n",
    "    except Exception:\n",
    "        return \"ERROR\"\n",
    "    if r.status_code == 404:\n",
    "        return \"HARD_404\"\n",
    "    html = r.text.lower()\n",
    "    if any(sig in html for sig in SCB_SOFT_404_SIGNALS):\n",
    "        return \"SOFT_404\"\n",
    "    if r.status_code >= 400:\n",
    "        return r.status_code\n",
    "    return 200\n",
    "\n",
    "def do_request(url, timeout, use_get_stream):\n",
    "    \"\"\"\n",
    "    Enkelt request-wrapper: return (status, latency_seconds)\n",
    "    \"\"\"\n",
    "    start = time.time()\n",
    "    try:\n",
    "        if use_get_stream:\n",
    "            r = session.get(url, allow_redirects=True, timeout=timeout, stream=True)\n",
    "        else:\n",
    "            r = session.head(url, allow_redirects=True, timeout=timeout)\n",
    "        status = r.status_code\n",
    "    except Exception as e:\n",
    "        # kan skilja mellan timeout, connection error, etc.\n",
    "        status = \"ERROR\"\n",
    "    latency = time.time() - start\n",
    "    return status, latency\n",
    "\n",
    "# -------------------------\n",
    "# Adaptive concurrency primitives\n",
    "# -------------------------\n",
    "monitor_state = {\n",
    "    \"concurrency_limit\": CONFIG[\"initial_concurrency\"],\n",
    "    \"active_count\": 0,\n",
    "    \"recent_latencies\": [],   # list of float\n",
    "    \"recent_results\": [],     # list of (url, status)\n",
    "    \"lock\": threading.Lock(),\n",
    "    \"cond\": threading.Condition(threading.Lock()),\n",
    "    \"stopped\": False,\n",
    "    \"completed_count\": 0,\n",
    "}\n",
    "\n",
    "# Domain-level stats/backoff\n",
    "domain_stats = {}  # domain -> dict {failures, last_fail_time, backoff_until, total, ok, slow_count}\n",
    "\n",
    "def domain_info(domain):\n",
    "    if domain not in domain_stats:\n",
    "        domain_stats[domain] = {\"failures\":0, \"last_fail\":0, \"backoff_until\":0, \"total\":0, \"ok\":0, \"slow_count\":0}\n",
    "    return domain_stats[domain]\n",
    "\n",
    "# -------------------------\n",
    "# Load or build link list (checkpointing)\n",
    "# -------------------------\n",
    "def load_or_fetch_links():\n",
    "    # If checkpoint exists, load it; else, collect via your fetch_exturlusage loop\n",
    "    if os.path.exists(CONFIG[\"checkpoint_links\"]):\n",
    "        df_links = pd.read_csv(CONFIG[\"checkpoint_links\"])\n",
    "        print(f\"[Checkpoint] Loaded {len(df_links)} links from {CONFIG['checkpoint_links']}\")\n",
    "    else:\n",
    "        # --- Minimal implementation to build rows using the user's fetch_exturlusage ---\n",
    "        # You should replace this block with your own fetch_exturlusage(lang, domain) implementation\n",
    "        # For now we will read sources from files and create placeholder; you must provide fetch_exturlusage\n",
    "        langs = pd.read_csv(CONFIG[\"wiki_versions_file\"])[\"Language Code\"].dropna().tolist()\n",
    "        domains = pd.read_csv(CONFIG[\"domains_file\"], header=None)[0].dropna().tolist()\n",
    "        rows = []\n",
    "        print(\"Fetching exturlusage… (please ensure fetch_exturlusage is available in your namespace)\")\n",
    "        for lang in langs:\n",
    "            for domain in domains:\n",
    "                for item in fetch_exturlusage(lang, domain):\n",
    "                    rows.append(item)\n",
    "            # save incremental for safety\n",
    "            pd.DataFrame(rows).to_csv(CONFIG[\"checkpoint_links\"], index=False)\n",
    "        df_links = pd.DataFrame(rows)\n",
    "        print(f\"Saved checkpoint: {CONFIG['checkpoint_links']}\")\n",
    "    return df_links\n",
    "\n",
    "# -------------------------\n",
    "# Save status checkpoint\n",
    "# -------------------------\n",
    "def save_status_checkpoint(status_map):\n",
    "    df = pd.DataFrame([{\"url\":u, \"status\":s} for u,s in status_map.items()])\n",
    "    df.to_csv(CONFIG[\"checkpoint_status\"], index=False)\n",
    "\n",
    "# -------------------------\n",
    "# Worker logic\n",
    "# -------------------------\n",
    "def worker_task(url, tested_status):\n",
    "    \"\"\"\n",
    "    The worker will:\n",
    "     - check domain backoff\n",
    "     - wait for \"permission\" according to global concurrency_limit\n",
    "     - perform request(s)\n",
    "     - update monitor_state (latency + result)\n",
    "     - return (url, status)\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    domain = parsed.netloc.lower()\n",
    "\n",
    "    # If domain is in backoff, skip\n",
    "    dinfo = domain_info(domain)\n",
    "    now = time.time()\n",
    "    if dinfo.get(\"backoff_until\",0) > now:\n",
    "        return url, \"SKIPPED_BACKOFF\"\n",
    "\n",
    "    # Wait for global concurrency permission\n",
    "    cond = monitor_state[\"cond\"]\n",
    "    with cond:\n",
    "        while monitor_state[\"active_count\"] >= monitor_state[\"concurrency_limit\"]:\n",
    "            cond.wait(timeout=0.5)\n",
    "        monitor_state[\"active_count\"] += 1\n",
    "\n",
    "    # perform request\n",
    "    status = None\n",
    "    latency = None\n",
    "    try:\n",
    "        # small per-domain polite sleep if too many recent calls for same domain\n",
    "        # (very simple, not a full rate-limiter)\n",
    "        time.sleep(0.0)\n",
    "\n",
    "        attempts = 0\n",
    "        while attempts <= CONFIG[\"max_retries\"]:\n",
    "            attempts += 1\n",
    "            status, latency = do_request(url, CONFIG[\"request_timeout\"], CONFIG[\"use_get_stream\"])\n",
    "            # Special SCB detection if scb in url and we got 200 -> run detect\n",
    "            if \"scb.se\" in url.lower() and status == 200:\n",
    "                status = detect_scb_soft_404(url)\n",
    "            # Accept non-200 numeric codes as final; for \"ERROR\" we may retry once\n",
    "            if status != \"ERROR\":\n",
    "                break\n",
    "        # update domain stats\n",
    "        dinfo = domain_info(domain)\n",
    "        dinfo[\"total\"] += 1\n",
    "        if status == 200:\n",
    "            dinfo[\"ok\"] += 1\n",
    "            dinfo[\"failures\"] = 0\n",
    "        elif status == \"SKIPPED_BACKOFF\":\n",
    "            pass\n",
    "        else:\n",
    "            dinfo[\"failures\"] += 1\n",
    "            dinfo[\"last_fail\"] = time.time()\n",
    "            # set backoff if threshold exceeded\n",
    "            if dinfo[\"failures\"] >= CONFIG[\"per_domain_fail_threshold\"]:\n",
    "                backoff_seconds = CONFIG[\"per_domain_backoff_base\"] * (2 ** (dinfo[\"failures\"] - CONFIG[\"per_domain_fail_threshold\"]))\n",
    "                backoff_seconds = min(backoff_seconds, 3600)  # cap to 1h\n",
    "                dinfo[\"backoff_until\"] = time.time() + backoff_seconds\n",
    "\n",
    "    finally:\n",
    "        # update monitor_state\n",
    "        with monitor_state[\"lock\"]:\n",
    "            monitor_state[\"recent_results\"].append((url, status, latency))\n",
    "            if latency is not None:\n",
    "                monitor_state[\"recent_latencies\"].append(latency)\n",
    "            # keep rolling window\n",
    "            if len(monitor_state[\"recent_latencies\"]) > CONFIG[\"rolling_window\"]:\n",
    "                monitor_state[\"recent_latencies\"] = monitor_state[\"recent_latencies\"][-CONFIG[\"rolling_window\"]:]\n",
    "            if len(monitor_state[\"recent_results\"]) > CONFIG[\"rolling_window\"]:\n",
    "                monitor_state[\"recent_results\"] = monitor_state[\"recent_results\"][-CONFIG[\"rolling_window\"]:]\n",
    "            monitor_state[\"completed_count\"] += 1\n",
    "\n",
    "        # release active slot\n",
    "        cond = monitor_state[\"cond\"]\n",
    "        with cond:\n",
    "            monitor_state[\"active_count\"] -= 1\n",
    "            cond.notify_all()\n",
    "\n",
    "    return url, status\n",
    "\n",
    "# -------------------------\n",
    "# Monitor thread — adapts concurrency + prints stats\n",
    "# -------------------------\n",
    "def monitor_thread_fn(total_to_check, tested_map):\n",
    "    last_print = 0\n",
    "    while not monitor_state[\"stopped\"]:\n",
    "        time.sleep(CONFIG[\"monitor_interval\"])\n",
    "        with monitor_state[\"lock\"]:\n",
    "            latencies = monitor_state[\"recent_latencies\"][:]\n",
    "            results = monitor_state[\"recent_results\"][:]\n",
    "            completed = monitor_state[\"completed_count\"]\n",
    "            active = monitor_state[\"active_count\"]\n",
    "            curr_limit = monitor_state[\"concurrency_limit\"]\n",
    "\n",
    "        # compute metrics\n",
    "        avg_latency = (sum(latencies)/len(latencies)) if latencies else 0.0\n",
    "        err_count = sum(1 for (_,s,_) in results if s not in (200, \"SKIPPED_BACKOFF\", None))\n",
    "        recent = len(results)\n",
    "        err_rate = (err_count / recent) if recent else 0.0\n",
    "        pct_done = (completed / total_to_check * 100) if total_to_check else 0.0\n",
    "\n",
    "        # adapt concurrency\n",
    "        changed = False\n",
    "        if avg_latency > CONFIG[\"target_avg_latency\"] and curr_limit > CONFIG[\"min_concurrency\"]:\n",
    "            # decrease\n",
    "            new_limit = max(CONFIG[\"min_concurrency\"], curr_limit - CONFIG[\"decrease_step\"])\n",
    "            monitor_state[\"concurrency_limit\"] = new_limit\n",
    "            changed = True\n",
    "        elif avg_latency <= CONFIG[\"target_avg_latency\"] and curr_limit < CONFIG[\"max_workers_cap\"]:\n",
    "            # increase gently\n",
    "            new_limit = min(CONFIG[\"max_workers_cap\"], curr_limit + CONFIG[\"increase_step\"])\n",
    "            monitor_state[\"concurrency_limit\"] = new_limit\n",
    "            changed = True\n",
    "\n",
    "        # Print concise status line\n",
    "        slow_domains = sorted(domain_stats.items(), key=lambda kv: kv[1].get(\"failures\",0), reverse=True)[:5]\n",
    "        slow_summary = \", \".join([f\"{d}:{s['failures']}\" for d,s in slow_domains if s['failures']>0]) or \"none\"\n",
    "        print(f\"[Monitor] done={completed}/{total_to_check} ({pct_done:.1f}%) active={active} limit={monitor_state['concurrency_limit']} avg_lat={avg_latency:.2f}s err_rate={err_rate:.2%} slow_domains={slow_summary}\")\n",
    "        if changed:\n",
    "            # wake waiting tasks so they re-check concurrency_limit\n",
    "            with monitor_state[\"cond\"]:\n",
    "                monitor_state[\"cond\"].notify_all()\n",
    "\n",
    "        # save status checkpoint occasionally\n",
    "        if completed % CONFIG[\"status_save_every\"] == 0 and completed > 0:\n",
    "            # build simple tested_map from recent_results + existing\n",
    "            with monitor_state[\"lock\"]:\n",
    "                for (u,s,_) in monitor_state[\"recent_results\"]:\n",
    "                    tested_map[u] = s\n",
    "            save_status_checkpoint(tested_map)\n",
    "\n",
    "# -------------------------\n",
    "# High-level runner\n",
    "# -------------------------\n",
    "def run_audit():\n",
    "    # 1) load links (or fetch)\n",
    "    df_links = load_or_fetch_links()\n",
    "    print(f\"Total links (raw): {len(df_links)}\")\n",
    "    # dedupe\n",
    "    unique_urls = pd.Series(df_links[\"url\"].dropna().unique(), name=\"url\")\n",
    "    print(f\"Unique URLs to test: {len(unique_urls)}\")\n",
    "\n",
    "    # 2) load tested status checkpoint\n",
    "    tested_map = {}\n",
    "    if os.path.exists(CONFIG[\"checkpoint_status\"]):\n",
    "        s = pd.read_csv(CONFIG[\"checkpoint_status\"])\n",
    "        tested_map = dict(zip(s[\"url\"], s[\"status\"]))\n",
    "        print(f\"[Checkpoint] Loaded {len(tested_map)} tested statuses\")\n",
    "\n",
    "    # build list of urls to test (unique minus already tested)\n",
    "    remaining = [u for u in unique_urls.tolist() if u not in tested_map]\n",
    "    total_to_check = len(remaining)\n",
    "    print(f\"URLs to test (unique): {total_to_check}\")\n",
    "\n",
    "    # start monitor thread\n",
    "    monitor_t = threading.Thread(target=monitor_thread_fn, args=(total_to_check, tested_map), daemon=True)\n",
    "    monitor_t.start()\n",
    "\n",
    "    # threadpool for workers\n",
    "    max_workers = CONFIG[\"max_workers_cap\"]\n",
    "    futures = {}\n",
    "    pbar = tqdm(total=total_to_check, desc=\"Checking URLs\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        # submit all tasks but they internally obey concurrency_limit via condition\n",
    "        for url in remaining:\n",
    "            futures[ex.submit(worker_task, url, tested_map)] = url\n",
    "\n",
    "        # iterate as they complete to update pbar and tested_map\n",
    "        try:\n",
    "            for fut in as_completed(futures):\n",
    "                url = futures[fut]\n",
    "                try:\n",
    "                    u, status = fut.result()\n",
    "                except Exception as e:\n",
    "                    u, status = url, \"ERROR\"\n",
    "                tested_map[u] = status\n",
    "                pbar.update(1)\n",
    "\n",
    "                # periodic checkpoint\n",
    "                if monitor_state[\"completed_count\"] % CONFIG[\"status_save_every\"] == 0:\n",
    "                    save_status_checkpoint(tested_map)\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"User aborted. Saving checkpoint...\")\n",
    "            monitor_state[\"stopped\"] = True\n",
    "            save_status_checkpoint(tested_map)\n",
    "            raise\n",
    "        finally:\n",
    "            monitor_state[\"stopped\"] = True\n",
    "            pbar.close()\n",
    "\n",
    "    # merge back to full dataframe\n",
    "    df_links[\"status\"] = df_links[\"url\"].map(tested_map)\n",
    "    df_links.to_csv(CONFIG[\"out_all_links\"], index=False)\n",
    "    dead_df = df_links[df_links[\"status\"].isin([\"HARD_404\", \"SOFT_404\", \"ERROR\"])]\n",
    "    dead_df.to_csv(CONFIG[\"out_dead_links\"], index=False)\n",
    "    save_status_checkpoint(tested_map)\n",
    "    print(\"Finished. Results saved to:\", CONFIG[\"out_all_links\"], CONFIG[\"out_dead_links\"])\n",
    "\n",
    "# -------------------------\n",
    "# Run!\n",
    "# -------------------------\n",
    "# NOTE: This cell will run in the notebook and print monitor lines periodically.\n",
    "# If you want non-blocking background execution, run it via subprocess from another cell.\n",
    "run_audit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5e942-32c4-42a4-8ce6-3bbfff2a2e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "\n",
    "st.title(\"SCB Link Audit — Wikipedia\")\n",
    "\n",
    "st.write(\"Visualisering av länkröta relaterad till scb.se på Wikipedia.\")\n",
    "\n",
    "df_all = pd.read_csv(\"results/all_links.csv\")\n",
    "df_dead = pd.read_csv(\"results/dead_links.csv\")\n",
    "df_art = pd.read_csv(\"results/stats_per_article.csv\")\n",
    "df_path = pd.read_csv(\"results/stats_per_domain_path.csv\")\n",
    "\n",
    "st.header(\"1. Översikt\")\n",
    "col1, col2, col3 = st.columns(3)\n",
    "\n",
    "col1.metric(\"Totala länkar\", len(df_all))\n",
    "col2.metric(\"Döda länkar\", len(df_dead))\n",
    "col3.metric(\"Döda (%)\", f\"{100 * len(df_dead)/len(df_all):.2f}%\")\n",
    "\n",
    "st.header(\"2. Status-fördelning\")\n",
    "st.bar_chart(df_all[\"status\"].value_counts())\n",
    "\n",
    "st.header(\"3. Artiklar med flest trasiga länkar\")\n",
    "bad = df_art[df_art[\"dead_links\"] > 0].sort_values(\"dead_links\", ascending=False)\n",
    "st.dataframe(bad)\n",
    "\n",
    "st.header(\"4. SCB paths där flest länkar gått sönder\")\n",
    "st.dataframe(df_path.sort_values(\"dead_ratio\", ascending=False))\n",
    "\n",
    "st.header(\"5. Alla länkar\")\n",
    "st.dataframe(df_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75daac4d-42af-48f4-a602-ba1b6c7e106c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # End timer and calculate duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time# Bygg audit-lager för den här etappen\n",
    "\n",
    "# Print current date and total time\n",
    "print(\"Date:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "print(\"Total time elapsed: {:02.0f} minutes {:05.2f} seconds\".format(minutes, seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c251a2-4190-4a21-aba7-eeff4114f68a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
