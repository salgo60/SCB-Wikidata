{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d00f9b4d-3480-4da6-8f5a-80a72c7f4f4a",
   "metadata": {},
   "source": [
    "* [#4](https://github.com/salgo60/SCB-Wikidata/issues/4)\n",
    "* denna [SCB4_linkroot_all.ipynb](https://github.com/salgo60/SCB-Wikidata/blob/main/notebook/SCB4_linkroot_all.ipynb)\n",
    "\n",
    "Tanken är att kolla alla Wikipedia domäner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "06856c50-f00c-4b5e-9093-0419979115be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2025-11-17 09:47:55\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "timestamp = now.timestamp()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Start:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d30d637c-ea85-43bb-830d-7b6e0dee6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_language_codes_auto():\n",
    "    \"\"\"\n",
    "    Load all known Wikipedia language codes.\n",
    "    \"\"\"\n",
    "    url = \"https://meta.wikimedia.org/wiki/Special:SiteMatrix?uselang=en\"\n",
    "    from bs4 import BeautifulSoup\n",
    "    import requests\n",
    "\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    codes = set()\n",
    "    for link in soup.select(\"a\"):\n",
    "        href = link.get(\"href\", \"\")\n",
    "        if href.startswith(\"//\") and \".wikipedia.org\" in href:\n",
    "            lang = href.split(\"//\")[1].split(\".\")[0]\n",
    "            codes.add(lang)\n",
    "\n",
    "    return sorted(codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb6cda25-92b4-4376-b047-03a770c30e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def read_domains(file_path): \n",
    "    print(f\"[DEBUG] Reading domains from: {file_path}\") \n",
    "    df = pd.read_csv(file_path, header=None, names=['domain']) \n",
    "    domains_list = df['domain'].dropna().unique().tolist() \n",
    "    print(f\"[DEBUG] Found {len(domains_list)} domains.\") \n",
    "    return domains_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a64cb8f-9c7c-478f-97f5-05c7c6aa48de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c12c809-f6b4-4abe-91b2-0483b895ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "CHECKPOINT_LINKS_ALL  = \"results_all/_checkpoint_exturlusage_all.csv\"\n",
    "CHECKPOINT_STATUS_ALL = \"results_all/_checkpoint_status_all.csv\"\n",
    "# -------------------------\n",
    "# CONFIG — justera vid behov\n",
    "# -------------------------\n",
    "CONFIG = {\n",
    "    \"checkpoint_links_all\": \"results_all/_checkpoint_exturlusage_all.csv\",\n",
    "    \"checkpoint_status_all\": \"results_all/_checkpoint_status_all.csv\",\n",
    "    \"out_all_links\": \"results_all/all_links.csv\",\n",
    "}\n",
    "\n",
    "os.makedirs(\"results\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "59556736-61d4-46bb-8ca3-e21b5d919819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# Fetch exturlusage entries for one lang/domain\n",
    "# -----------------------------------------------------------\n",
    "def fetch_exturlusage(lang, domain):\n",
    "    base = f\"https://{lang}.wikipedia.org/w/api.php\"\n",
    "    params = {\n",
    "        \"action\": \"query\",\n",
    "        \"format\": \"json\",\n",
    "        \"list\": \"exturlusage\",\n",
    "        \"euquery\": domain,\n",
    "        \"eulimit\": \"max\"\n",
    "    }\n",
    "    while True:\n",
    "        r = session.get(base, params=params, timeout=10)\n",
    "        data = r.json()\n",
    "        for item in data.get(\"query\", {}).get(\"exturlusage\", []):\n",
    "            yield {\n",
    "                \"lang\": lang,\n",
    "                \"page_title\": item.get(\"title\"),\n",
    "                \"url\": item.get(\"url\"),\n",
    "                \"wiki_link\": f\"https://{lang}.wikipedia.org/wiki/{item.get('title').replace(' ', '_')}\"\n",
    "            }\n",
    "\n",
    "        if \"continue\" not in data:\n",
    "            break\n",
    "        params.update(data[\"continue\"])\n",
    "        time.sleep(0.3)\n",
    "# -----------------------------------------------------------\n",
    "# Modified main() with resume support\n",
    "# -----------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac4c77-f7d9-48f4-8b6c-d3b1663bcdde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b041536a-abda-4d22-bda9-9c3407b94bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188a9514-382f-470c-9158-5ed2553ebabe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "378f599c-99ca-4ecd-90ad-e860e847ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "MAX_WORKERS             = 5\n",
    "SLEEP_BETWEEN_REQUESTS  = 1\n",
    "USER_CONTRIB_LIMIT      = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "12ed5b21-fa18-42b3-b890-69f78911eec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] Reading domains from: sources/domains.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fd/md6r13sj0wsbg_6_xl160d300000gn/T/ipykernel_66257/3932524435.py:10: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(r.text, \"html.parser\")\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'domain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'domain'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 89\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DEBUG] All (lang, domain) pairs processed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 89\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[77], line 64\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Read language codes & domains\u001b[39;00m\n\u001b[1;32m     63\u001b[0m language_codes \u001b[38;5;241m=\u001b[39m read_language_codes_auto()\n\u001b[0;32m---> 64\u001b[0m domains \u001b[38;5;241m=\u001b[39m \u001b[43mread_domains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDOMAINS_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(domains)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[DEBUG] Starting concurrency for (lang, domain) pairs...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[77], line 17\u001b[0m, in \u001b[0;36mread_domains\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdomain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mdropna()\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'domain'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "MAIN_RESULTS_OUTPUT_ALL = \"results_all/results_all.csv\"\n",
    "WIKI_VERSIONS_FILE_ALL = \"sources/wiki_versions_all.csv\"\n",
    "DOMAINS_FILE = \"sources/domains.csv\"\n",
    "\n",
    "MAX_WORKERS = 20\n",
    "\n",
    "\n",
    "def read_domains(file_path):\n",
    "    print(f\"[DEBUG] Reading domains from: {file_path}\")\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"Missing file: {file_path}\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['domain'].dropna().unique().tolist()\n",
    "\n",
    "\n",
    "def process_lang_domain_pair(lang, domain, writer):\n",
    "    \"\"\"\n",
    "    Dummy example of URL checking.\n",
    "    Replace with your real logic.\n",
    "    \"\"\"\n",
    "    url = f\"https://{lang}.{domain}\"\n",
    "    print(f\"[DEBUG] Checking URL: {url}\")\n",
    "\n",
    "    try:\n",
    "        import requests\n",
    "        r = requests.get(url, timeout=5)\n",
    "        status = \"OK\" if r.status_code == 200 else r.status_code\n",
    "    except Exception as e:\n",
    "        status = f\"ERROR: {e}\"\n",
    "\n",
    "    writer.writerow({\n",
    "        \"lang\": lang,\n",
    "        \"domain\": domain,\n",
    "        \"url\": url,\n",
    "        \"page_title\": \"\",\n",
    "        \"wiki_link\": \"\",\n",
    "        \"user\": \"\",\n",
    "        \"timestamp\": status\n",
    "    })\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    os.makedirs(\"results_all\", exist_ok=True)\n",
    "\n",
    "    # Will the CSV need a header?\n",
    "    write_header = not os.path.exists(MAIN_RESULTS_OUTPUT_ALL)\n",
    "\n",
    "    main_fields = ['lang', 'domain', 'url', 'page_title',\n",
    "                   'wiki_link', 'user', 'timestamp']\n",
    "\n",
    "    with open(MAIN_RESULTS_OUTPUT_ALL, 'a', newline='', encoding='utf-8') as main_out:\n",
    "\n",
    "        writer = csv.DictWriter(main_out, fieldnames=main_fields)\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "\n",
    "        # Read language codes & domains\n",
    "        language_codes = read_language_codes_auto()\n",
    "        domains = read_domains(DOMAINS_FILE)\n",
    "\n",
    "        print(domains)\n",
    "\n",
    "        print(\"[DEBUG] Starting concurrency for (lang, domain) pairs...\")\n",
    "\n",
    "        futures = []\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "            for lang in language_codes:\n",
    "                for domain in domains:\n",
    "                    futures.append(executor.submit(\n",
    "                        process_lang_domain_pair, lang, domain, writer\n",
    "                    ))\n",
    "\n",
    "            for fut in as_completed(futures):\n",
    "                try:\n",
    "                    fut.result()\n",
    "                except Exception as e:\n",
    "                    print(\"[ERROR] Worker error:\", e)\n",
    "\n",
    "    print(f\"[DEBUG] Done collecting main results in {MAIN_RESULTS_OUTPUT_ALL}\")\n",
    "    print(\"[DEBUG] All (lang, domain) pairs processed.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b61e0018-50e7-401a-9d7a-7a1573e96808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alla rader i checkpoint: 0\n",
      "Totalt unika URL: 0\n",
      "\n",
      "Språk-kolumn hittad: 'lang'\n",
      "\n",
      "--- Statistik per språk (alla rader) ---\n",
      "\n",
      "--- Statistik per språk (unika URL:er) ---\n",
      "\n",
      "Antal språk: 0\n",
      "\n",
      "--- Statistik per domän (flest först) ---\n",
      "\n",
      "Antal domäner: 0\n",
      "\n",
      "Laddade tidigare status: 0\n",
      "\n",
      "--- Sammanfattning ---\n",
      "Totalt unika URL:         0\n",
      "Antal domäner:             0\n",
      "\n",
      "Topp 100 domäner:\n",
      "Series([], Name: count, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time \n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "# -------------------------------------------------\n",
    "# LADDA LÄNKAR\n",
    "# -------------------------------------------------\n",
    "df_all = pd.read_csv(CHECKPOINT_LINKS_ALL)\n",
    "\n",
    "print(\"Alla rader i checkpoint:\", len(df_all))\n",
    "\n",
    "# Ta unika URL:er\n",
    "urls = df_all[\"url\"].dropna().unique().tolist()\n",
    "total = len(urls)\n",
    "print(f\"Totalt unika URL: {total}\")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# LÄNKAR PER WIKIPEDIA-SPRÅK\n",
    "# -------------------------------------------------\n",
    "\n",
    "# Försök hitta vilken kolumn som innehåller språkkoden\n",
    "possible_lang_cols = [\"lang\", \"language\", \"wiki\", \"wiki_lang\", \"language_code\"]\n",
    "lang_col = None\n",
    "\n",
    "for c in possible_lang_cols:\n",
    "    if c in df_all.columns:\n",
    "        lang_col = c\n",
    "        break\n",
    "\n",
    "if lang_col is None:\n",
    "    print(\"⚠️ Hittar ingen språk-kolumn i datan! Inga språk-stats kan visas.\")\n",
    "else:\n",
    "    print(f\"\\nSpråk-kolumn hittad: '{lang_col}'\")\n",
    "\n",
    "    # Totalt antal länkar per språk (inkl. dubbletter)\n",
    "    print(\"\\n--- Statistik per språk (alla rader) ---\")\n",
    "    lang_counts_all = df_all[lang_col].value_counts()\n",
    "    for lang, cnt in lang_counts_all.items():\n",
    "        print(f\"{lang:5} {cnt}\")\n",
    "\n",
    "    # Antal UNIKA URLer per språk\n",
    "    print(\"\\n--- Statistik per språk (unika URL:er) ---\")\n",
    "    df_all_unique = df_all.drop_duplicates(subset=[\"url\"])\n",
    "    lang_counts_unique_all = df_all_unique[lang_col].value_counts()\n",
    "    for lang, cnt in lang_counts_unique_all.items():\n",
    "        print(f\"{lang:5} {cnt}\")\n",
    "\n",
    "    print(\"\\nAntal språk:\", len(lang_counts_all))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# DOMÄNSTATISTIK\n",
    "# -------------------------------------------------\n",
    "domains = [urlparse(u).netloc.lower() for u in urls]\n",
    "domain_counts = pd.Series(domains).value_counts()\n",
    "\n",
    "print(\"\\n--- Statistik per domän (flest först) ---\")\n",
    "for dom, cnt in domain_counts.items():\n",
    "    print(f\"{dom:35} {cnt}\")\n",
    "\n",
    "print(\"\\nAntal domäner:\", len(domain_counts))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# LADDA PREVIOUS STATUS (resume)\n",
    "# -------------------------------------------------\n",
    "status_map = {}\n",
    "if os.path.exists(CHECKPOINT_STATUS_ALL):\n",
    "    prev_all = pd.read_csv(CHECKPOINT_STATUS_ALL)\n",
    "    status_map_all = dict(zip(prev_all[\"url\"], prev_all[\"status\"]))\n",
    "    print(\"\\nLaddade tidigare status:\", len(status_map))\n",
    "\n",
    "# Filtrera bort redan testade länkar\n",
    "remaining = [u for u in urls if u not in status_map]\n",
    "\n",
    "print(\"\\n--- Sammanfattning ---\")\n",
    "print(f\"Totalt unika URL:         {total}\")\n",
    "print(f\"Antal domäner:             {len(domain_counts)}\")\n",
    "\n",
    "# Visa topp 10 domäner\n",
    "print(\"\\nTopp 100 domäner:\")\n",
    "print(domain_counts.head(100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a50c1cc9-99fe-4b05-a63e-3e6ec1347a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2025-11-16 11:11:24\n",
      "Total time elapsed: 08 minutes 19.46 seconds\n"
     ]
    }
   ],
   "source": [
    " # End timer and calculate duration\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time# Bygg audit-lager för den här etappen\n",
    "\n",
    "# Print current date and total time\n",
    "print(\"Date:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "minutes, seconds = divmod(elapsed_time, 60)\n",
    "print(\"Total time elapsed: {:02.0f} minutes {:05.2f} seconds\".format(minutes, seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bdb35c-d6d3-473a-ae9c-93b7c1d73393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
